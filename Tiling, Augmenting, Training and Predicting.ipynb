{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Libraries and create output folders\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install tifffile\n",
    "!{sys.executable} -m pip install opencv-python-headless\n",
    "!{sys.executable} -m pip install keras==2.2.4\n",
    "!{sys.executable} -m pip install imutils\n",
    "!{sys.executable} -m pip install scipy\n",
    "!{sys.executable} -m pip install scikit-image\n",
    "!{sys.executable} -m pip install rasterio\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"logs\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(\"weights\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "from random import randint\n",
    "\n",
    "import random\n",
    "random.seed(1337)\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1337)\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tifffile as tiff\n",
    "import rasterio\n",
    "from keras import backend\n",
    "from tensorflow import ones_like, equal, log\n",
    "from trainvaltensorboard import TrainValTensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Input, Conv2D, AlphaDropout, MaxPooling2D, Conv2DTranspose, BatchNormalization, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.metrics import mean_squared_error\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib.pyplot import figure\n",
    "from scipy import ndimage, misc\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Planet images\n",
    "\n",
    "img_labels_in_30 = tiff.imread(\"binary_30.tif\")\n",
    "img_labels_in_27 = tiff.imread(\"binary_27.tif\")\n",
    "img_labels_in_23 = tiff.imread(\"binary_23.tif\")\n",
    "img_labels_in_30 = img_labels_in_30[:,:,3] == 255\n",
    "img_labels_in_27 = img_labels_in_27[:,:,3] == 255\n",
    "img_labels_in_23 = img_labels_in_23[:,:,3] == 255\n",
    "img_labels_in_30 = img_labels_in_30[0:3334,0:5500] == 0\n",
    "img_labels_in_27 = img_labels_in_27[0:3334,0:5500] == 0\n",
    "img_labels_in_23 = img_labels_in_23[0:3334,0:5500] == 0\n",
    "\n",
    "img_data_in_23 = rasterio.open(\"Mosaic_20170823_DHDN_clip.tif\")\n",
    "img_data_in_23 = np.concatenate((np.expand_dims(img_data_in_23.read(1), axis=2),np.expand_dims(img_data_in_23.read(2), axis=2),np.expand_dims(img_data_in_23.read(3), axis=2),np.expand_dims(img_data_in_23.read(4), axis=2)), axis=2)\n",
    "img_data_in_27 = rasterio.open(\"Mosaic_20170827_DHDN_clip.tif\")\n",
    "img_data_in_27 = np.concatenate((np.expand_dims(img_data_in_27.read(1), axis=2),np.expand_dims(img_data_in_27.read(2), axis=2),np.expand_dims(img_data_in_27.read(3), axis=2),np.expand_dims(img_data_in_27.read(4), axis=2)), axis=2)\n",
    "img_data_in_30 = rasterio.open(\"Mosaic_20170830_DHDN_clip.tif\")\n",
    "img_data_in_30 = np.concatenate((np.expand_dims(img_data_in_30.read(1), axis=2),np.expand_dims(img_data_in_30.read(2), axis=2),np.expand_dims(img_data_in_30.read(3), axis=2),np.expand_dims(img_data_in_30.read(4), axis=2)), axis=2)\n",
    "img_labels_in = np.concatenate((np.expand_dims(img_labels_in_30, axis=0), np.expand_dims(img_labels_in_27, axis=0), np.expand_dims(img_labels_in_23, axis=0)), axis=0)\n",
    "img_data_in = np.concatenate((np.expand_dims(img_data_in_30[0:3334,0:5500,:], axis=0), np.expand_dims(img_data_in_27[0:3334,0:5500,:], axis=0), np.expand_dims(img_data_in_23[0:3334,0:5500,:], axis=0)), axis=0)\n",
    "img_data_in = (img_data_in / 65536).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ortho tiles\n",
    "\n",
    "val_data = [\n",
    "    \"4606000-5396000\",\n",
    "    \"4608000-5390000\",\n",
    "    \"4610000-5394000\",\n",
    "    \"4614000-5392000\",\n",
    "    \"4614000-5388000\",\n",
    "    \"4616000-5394000\",\n",
    "    \"4618000-5390000\",\n",
    "    \"4622000-5392000\"\n",
    "]\n",
    "data = glob(\"Orthos/*/\")\n",
    "train_data = []\n",
    "for d in data:\n",
    "    if(d.split(\"/\")[1] not in val_data):\n",
    "        train_data.append(d)\n",
    "data_tiles_list = []\n",
    "label_tiles_list = []\n",
    "for p in train_data:\n",
    "    data_tiles_list.append(glob(p + \"/images/*.tif\"))\n",
    "    label_tiles_list.append(glob(p + \"/labels/0/*.tif\"))\n",
    "\n",
    "data_tiles_list = [y for x in data_tiles_list for y in x]\n",
    "label_tiles_list = [y for x in label_tiles_list for y in x]\n",
    "\n",
    "training_images = []\n",
    "for i in range(len(data_tiles_list)):\n",
    "    print(str(i) + \"/\" + str(len(data_tiles_list)))\n",
    "    data_in = rasterio.open(data_tiles_list[i])\n",
    "    training_images.append(np.concatenate((np.expand_dims(data_in.read(1), axis=2),np.expand_dims(data_in.read(2), axis=2),np.expand_dims(data_in.read(3), axis=2),np.expand_dims(data_in.read(4), axis=2)), axis=2))\n",
    "training_images = np.asarray(training_images)\n",
    "training_images = (training_images / 256).astype(np.float32)\n",
    "print(training_images.shape)\n",
    "\n",
    "training_labels = []\n",
    "for i in range(len(label_tiles_list)):\n",
    "    print(str(i) + \"/\" + str(len(label_tiles_list)))\n",
    "    data_in = data_in = rasterio.open(label_tiles_list[i])\n",
    "    training_labels.append(data_in.read(1))\n",
    "training_labels = np.asarray(training_labels)\n",
    "training_labels = np.expand_dims(training_labels, axis=3)\n",
    "training_labels = training_labels == 0\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ortho tiles without damage and create empty labeling, then merge the tiles with the Ortho tiles\n",
    "\n",
    "data = glob(\"Orthos NoDamage/*/\")\n",
    "train = []\n",
    "data_tiles_list_s = []\n",
    "label_tiles_list_s = []\n",
    "for p in data:\n",
    "    data_tiles_list_s.append(glob(p + \"/images/*.tif\"))\n",
    "    label_tiles_list_s.append(glob(p + \"/labels/0/*.tif\"))\n",
    "\n",
    "data_tiles_list_s = [y for x in data_tiles_list_s for y in x]\n",
    "label_tiles_list_s = [y for x in label_tiles_list_s for y in x]\n",
    "\n",
    "training_images_s = []\n",
    "for i in range(len(data_tiles_list_s)):\n",
    "    print(str(i) + \"/\" + str(len(data_tiles_list_s)))\n",
    "    data_in = rasterio.open(data_tiles_list_s[i])\n",
    "    training_images_s.append(np.concatenate((np.expand_dims(data_in.read(1), axis=2),np.expand_dims(data_in.read(2), axis=2),np.expand_dims(data_in.read(3), axis=2),np.expand_dims(data_in.read(4), axis=2)), axis=2))\n",
    "training_images_s = np.asarray(training_images_s)\n",
    "training_images_s = (training_images_s / 256).astype(np.float32)\n",
    "print(training_images_s.shape)\n",
    "\n",
    "training_labels_s = []\n",
    "for i in range(training_images_s.shape[0]):\n",
    "    t = np.zeros((256,256))\n",
    "    training_labels_s.append(t)\n",
    "training_labels_s = np.expand_dims(np.asarray(training_labels_s), axis=3)\n",
    "training_labels_s = training_labels_s == 0\n",
    "print(training_labels_s.shape)\n",
    "\n",
    "training_images_s2 = training_images_s\n",
    "training_labels_s2 = training_labels_s\n",
    "\n",
    "training_images = np.concatenate((training_images, training_images_s2), axis=0)\n",
    "training_labels = np.concatenate((training_labels, training_labels_s2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Planet label image\n",
    "\n",
    "figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.imshow(img_labels_in[0,:,:].astype(np.uint8), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Planet mage\n",
    "\n",
    "figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.imshow((img_data_in_noDamage[0][:,:,0:3]*256).astype(np.uint8)*6)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planet Image and label tiling, then data augmentation\n",
    "\n",
    "rotation = [1,3] # Rot 180 can be produced by HOR+VER Flip!\n",
    "flip_horizontal = True\n",
    "flip_vertical = True\n",
    "validation_split = 0.235 # fit 3 rows of test at bottom\n",
    "validation_split = 0.154 # fit 2 rows of test at bottom\n",
    "tile_size_x = 256\n",
    "tile_size_y = 256\n",
    "stride_x = 128\n",
    "stride_y = 128\n",
    "random_select = False\n",
    "num_color_images = 0\n",
    "sigma = 1.5\n",
    "\n",
    "split_height = int(img_data_in.shape[1] * validation_split)\n",
    "img_data_in_d = img_data_in[:,:-split_height,:,:]\n",
    "img_labels_in_d = img_labels_in[:,:-split_height,:]\n",
    "img_data_in_v = img_data_in[:,-split_height:,:,:]\n",
    "img_labels_in_v = img_labels_in[:,-split_height:,:]\n",
    "print(\"Validation Split: \" + str(img_data_in_d.shape[1]) + \"/\" + str(img_data_in_v.shape[1]))\n",
    "\n",
    "labels_training = []\n",
    "images_training = []\n",
    "for o in range(img_labels_in_d.shape[0]):\n",
    "    i = 0\n",
    "    while i <= img_labels_in_d[o].shape[0] - tile_size_y:\n",
    "        j = 0\n",
    "        while j <= img_labels_in_d[o].shape[1] - tile_size_x:\n",
    "            tile = img_labels_in_d[o, i:i+tile_size_y, j:j+tile_size_x]\n",
    "            if np.sum(tile) > 0:\n",
    "                tile2 = img_data_in_d[o, i:i+tile_size_y, j:j+tile_size_x]\n",
    "                if np.min(tile2 > 0):\n",
    "                    labels_training.append(tile)\n",
    "                    images_training.append(tile2)\n",
    "            j += tile_size_x - stride_x\n",
    "        i += tile_size_y - stride_y\n",
    "    \n",
    "training_labels = np.expand_dims(np.asarray(labels_training), axis=3) == 0\n",
    "training_images = np.asarray(images_training)\n",
    "\n",
    "labels_validation = []\n",
    "images_validation = []\n",
    "for o in range(img_labels_in_v.shape[0]):\n",
    "    i = 0\n",
    "    while i <= img_labels_in_v[o].shape[0] - tile_size_y:\n",
    "        j = 0\n",
    "        while j <= img_labels_in_v[o].shape[1] - tile_size_x:\n",
    "            tile = img_labels_in_v[o, i:i+tile_size_y, j:j+tile_size_x]\n",
    "            if np.sum(tile) > 0:\n",
    "                tile2 = img_data_in_v[o, i:i+tile_size_y, j:j+tile_size_x]\n",
    "                if np.min(tile2 > 0):\n",
    "                    labels_validation.append(tile)\n",
    "                    images_validation.append(tile2)\n",
    "            j += tile_size_x# - stride_x\n",
    "        i += tile_size_y# - stride_y\n",
    "\n",
    "validation_labels = np.expand_dims(np.asarray(labels_validation), axis=3) == 0\n",
    "validation_images = np.asarray(images_validation)\n",
    "\n",
    "print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "print(\"Number of validation labels/images: \" + str(validation_labels.shape))\n",
    "\n",
    "if str(random_select).isdigit():\n",
    "    random_select_split = int(training_images.shape[0] * random_select)\n",
    "    o = training_images.shape[0]\n",
    "    training_images = training_images[:random_select_split,:,:,:]\n",
    "    training_labels = training_labels[:random_select_split,:,:,:]\n",
    "    print(\"Random select tiles: \" + str(training_images.shape[0]) + \" of \" + str(o))\n",
    "\n",
    "if len(rotation) > 0:\n",
    "    training_images_rot = np.copy(training_images)\n",
    "    training_labels_rot = np.copy(training_labels)\n",
    "    for i in rotation:\n",
    "        training_images_rot = np.concatenate((training_images_rot, np.rot90(training_images, i, (1,2))), axis=0)\n",
    "        training_labels_rot = np.concatenate((training_labels_rot, np.rot90(training_labels, i, (1,2))), axis=0)\n",
    "    training_images = training_images_rot\n",
    "    training_labels = training_labels_rot\n",
    "    print(\"After rotation:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "\n",
    "if flip_horizontal:\n",
    "    training_images = np.concatenate((training_images, np.flip(training_images, 2)), axis=0)\n",
    "    training_labels = np.concatenate((training_labels, np.flip(training_labels, 2)), axis=0)\n",
    "    print(\"After horizontal flip:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "\n",
    "if flip_vertical:\n",
    "    training_images = np.concatenate((training_images, np.flip(training_images, 1)), axis=0)\n",
    "    training_labels = np.concatenate((training_labels, np.flip(training_labels, 1)), axis=0)\n",
    "    print(\"After vertical flip:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "    \n",
    "training_data = list(zip(training_images, training_labels))\n",
    "random.shuffle(training_data)\n",
    "training_images[:], training_labels[:] = zip(*training_data)\n",
    "\n",
    "if num_color_images > -1:\n",
    "    training_images6 = np.copy(training_images)\n",
    "    training_labels6 = np.copy(training_labels)\n",
    "    training_images3 = np.array(training_images)\n",
    "    training_labels3 = np.array(training_labels)\n",
    "    for i in range(num_color_images):\n",
    "        training_images3 = np.concatenate((training_images3, training_images), axis=0)\n",
    "        training_labels3 = np.concatenate((training_labels3, training_labels), axis=0)\n",
    "    training_images = training_images3\n",
    "    training_labels = training_labels3\n",
    "    for img in training_images:\n",
    "        img[:,:,0] = img[:,:,0] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,0] = np.where(img[:,:,0] < 1, img[:,:,0], 1)\n",
    "        img[:,:,0] = np.where(img[:,:,0] >= 0, img[:,:,0], 0)\n",
    "        img[:,:,1] = img[:,:,1] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,1] = np.where(img[:,:,1] < 1, img[:,:,1], 1)\n",
    "        img[:,:,1] = np.where(img[:,:,1] >= 0, img[:,:,1], 0)\n",
    "        img[:,:,2] = img[:,:,2] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,2] = np.where(img[:,:,2] < 1, img[:,:,2], 1)\n",
    "        img[:,:,2] = np.where(img[:,:,2] >= 0, img[:,:,2], 0)\n",
    "        img[:,:,3] = img[:,:,3] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,3] = np.where(img[:,:,3] < 1, img[:,:,3], 1)\n",
    "        img[:,:,3] = np.where(img[:,:,3] >= 0, img[:,:,3], 0)\n",
    "    training_images = np.concatenate((training_images, training_images6), axis=0)\n",
    "    training_labels = np.concatenate((training_labels, training_labels6), axis=0)\n",
    "    print(\"After color variations:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ortho data augmentation\n",
    "\n",
    "rotation = [1]\n",
    "flip_horizontal = True\n",
    "flip_vertical = False\n",
    "random_select = 1\n",
    "num_color_images = 1\n",
    "sigma = 1.5\n",
    "\n",
    "print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "\n",
    "if len(rotation) > 0:\n",
    "    training_images_rot = np.copy(training_images)\n",
    "    training_labels_rot = np.copy(training_labels)\n",
    "    for i in rotation:\n",
    "        training_images_rot = np.concatenate((training_images_rot, np.rot90(training_images, i, (1,2))), axis=0)\n",
    "        training_labels_rot = np.concatenate((training_labels_rot, np.rot90(training_labels, i, (1,2))), axis=0)\n",
    "    training_images = training_images_rot\n",
    "    training_labels = training_labels_rot\n",
    "    print(\"After rotation:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "\n",
    "if flip_horizontal:\n",
    "    training_images = np.concatenate((training_images, np.flip(training_images, 2)), axis=0)\n",
    "    training_labels = np.concatenate((training_labels, np.flip(training_labels, 2)), axis=0)\n",
    "    print(\"After horizontal flip:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "\n",
    "if flip_vertical:\n",
    "    training_images = np.concatenate((training_images, np.flip(training_images, 1)), axis=0)\n",
    "    training_labels = np.concatenate((training_labels, np.flip(training_labels, 1)), axis=0)\n",
    "    print(\"After vertical flip:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))\n",
    "    \n",
    "training_data = list(zip(training_images, training_labels))\n",
    "random.shuffle(training_data)\n",
    "training_images[:], training_labels[:] = zip(*training_data)\n",
    "\n",
    "if random_select > 0:\n",
    "    random_select_split = int(training_images.shape[0] * random_select)\n",
    "    training_images = training_images[:random_select_split,:,:,:]\n",
    "    training_labels = training_labels[:random_select_split,:,:,:]\n",
    "    print(\"Random select tiles: \" + str(training_images.shape[0]) + \" of \" + str(o))\n",
    "    \n",
    "if num_color_images > -1:\n",
    "    training_images6 = np.copy(training_images)\n",
    "    training_labels6 = np.copy(training_labels)\n",
    "    training_images3 = np.array(training_images)\n",
    "    training_labels3 = np.array(training_labels)\n",
    "    for i in range(num_color_images):\n",
    "        training_images3 = np.concatenate((training_images3, training_images), axis=0)\n",
    "        training_labels3 = np.concatenate((training_labels3, training_labels), axis=0)\n",
    "    training_images = training_images3\n",
    "    training_labels = training_labels3\n",
    "    for img in training_images:\n",
    "        img[:,:,0] = img[:,:,0] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,0] = np.where(img[:,:,0] < 1, img[:,:,0], 1)\n",
    "        img[:,:,0] = np.where(img[:,:,0] >= 0, img[:,:,0], 0)\n",
    "        img[:,:,1] = img[:,:,1] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,1] = np.where(img[:,:,1] < 1, img[:,:,1], 1)\n",
    "        img[:,:,1] = np.where(img[:,:,1] >= 0, img[:,:,1], 0)\n",
    "        img[:,:,2] = img[:,:,2] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,2] = np.where(img[:,:,2] < 1, img[:,:,2], 1)\n",
    "        img[:,:,2] = np.where(img[:,:,2] >= 0, img[:,:,2], 0)\n",
    "        img[:,:,3] = img[:,:,3] + np.random.normal(0, sigma) / 256\n",
    "        img[:,:,3] = np.where(img[:,:,3] < 1, img[:,:,3], 1)\n",
    "        img[:,:,3] = np.where(img[:,:,3] >= 0, img[:,:,3], 0)\n",
    "    training_images = np.concatenate((training_images, training_images6), axis=0)\n",
    "    training_labels = np.concatenate((training_labels, training_labels6), axis=0)\n",
    "    print(\"After color variations:\")\n",
    "    print(\"Number of training labels/images: \" + str(training_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some Planet images\n",
    "\n",
    "figure(num=None, figsize=(16, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int)*6)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.imshow(training_labels[r,:,:,0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int)*6)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.imshow(training_labels[r,:,:,0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int)*6)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.imshow(training_labels[r,:,:,0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "plt.subplot(2, 4, 7)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int)*6)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.imshow(training_labels[r,:,:,0])\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some Ortho images\n",
    "\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "print(np.min(training_labels[r,:,:,0]))\n",
    "print(np.max(training_labels[r,:,:,0]))\n",
    "print()\n",
    "plt.subplot(3, 4, 1)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(3, 4, 2)\n",
    "plt.imshow(training_labels[r,:,:,0], cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "print(np.min(training_labels[r,:,:,0]))\n",
    "print(np.max(training_labels[r,:,:,0]))\n",
    "print()\n",
    "plt.subplot(3, 4, 3)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(3, 4, 4)\n",
    "plt.imshow(training_labels[r,:,:,0], cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "print(np.min(training_labels[r,:,:,0]))\n",
    "print(np.max(training_labels[r,:,:,0]))\n",
    "print()\n",
    "plt.subplot(3, 4, 5)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(3, 4, 6)\n",
    "plt.imshow(training_labels[r,:,:,0], cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "print(np.min(training_labels[r,:,:,0]))\n",
    "print(np.max(training_labels[r,:,:,0]))\n",
    "print()\n",
    "plt.subplot(3, 4, 7)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(3, 4, 8)\n",
    "plt.imshow(training_labels[r,:,:,0], cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "print(np.min(training_labels[r,:,:,0]))\n",
    "print(np.max(training_labels[r,:,:,0]))\n",
    "print()\n",
    "plt.subplot(3, 4, 9)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(3, 4, 10)\n",
    "plt.imshow(training_labels[r,:,:,0], cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "r = randint(0, training_labels.shape[0] - 1)\n",
    "print(r)\n",
    "print(np.min(training_labels[r,:,:,0]))\n",
    "print(np.max(training_labels[r,:,:,0]))\n",
    "print()\n",
    "plt.subplot(3, 4, 11)\n",
    "plt.imshow((training_images[r,:,:,0:3]*256).astype(int))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(3, 4, 12)\n",
    "plt.imshow(training_labels[r,:,:,0], cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "bands = training_images.shape[3]\n",
    "image_height = training_images.shape[1]\n",
    "image_width = training_images.shape[2]\n",
    "\n",
    "if backend.image_data_format() == 'channels_first':\n",
    "    channel_axis = 1\n",
    "    input_shape = (bands, image_height, image_width)\n",
    "if backend.image_data_format() == 'channels_last':\n",
    "    channel_axis = 3\n",
    "    input_shape = (image_height, image_width, bands)\n",
    "inp = Input(input_shape)\n",
    "\n",
    "filter_size = (3, 3)\n",
    "blocks = [8,16,32,64,128]\n",
    "blocks2 = blocks\n",
    "activation = 'relu'\n",
    "filter_initializer = 'lecun_normal'\n",
    "\n",
    "#encoder\n",
    "encoder = inp\n",
    "encoder_list = []\n",
    "for block_id , n_block in enumerate(blocks):\n",
    "    with backend.name_scope('Encoder_block{0}'.format(block_id)):\n",
    "        encoder = Conv2D(filters = n_block, kernel_size = filter_size, activation = None, padding = 'same', kernel_initializer = filter_initializer) (encoder)\n",
    "        encoder = BatchNormalization(axis=channel_axis, momentum=0.9) (encoder)\n",
    "        encoder = Activation(activation) (encoder)\n",
    "        encoder = AlphaDropout(0, 1*block_id) (encoder)\n",
    "        encoder = Conv2D(filters = n_block, kernel_size = filter_size, dilation_rate = (2, 2), activation = None, padding='same', kernel_initializer = filter_initializer) (encoder)\n",
    "        encoder = BatchNormalization(axis=channel_axis, momentum=0.9) (encoder)\n",
    "        encoder = Activation(activation) (encoder)\n",
    "        encoder_list.append(encoder)\n",
    "        \n",
    "        #maxpooling between every 2 blocks\n",
    "        if block_id < len(blocks) - 1:\n",
    "            encoder = MaxPooling2D(pool_size = (2, 2)) (encoder)\n",
    "            \n",
    "#decoder\n",
    "decoder = encoder\n",
    "blocks = blocks[1:]\n",
    "for block_id, n_block in enumerate(blocks):\n",
    "    with backend.name_scope('Decoder_block_{0}'.format(block_id)):\n",
    "        block_id_inv = len(blocks) - block_id\n",
    "        decoder = concatenate([decoder, encoder_list[block_id_inv]], axis = channel_axis)\n",
    "        decoder = AlphaDropout(0, 1*block_id) (decoder)\n",
    "        decoder = Conv2D(filters = n_block, kernel_size = filter_size, activation = None, padding = 'same', kernel_initializer = filter_initializer) (decoder)\n",
    "#         decoder = BatchNormalization(axis=channel_axis, momentum=0.9) (decoder)\n",
    "        decoder = Activation(activation) (decoder)\n",
    "        decoder = Conv2DTranspose(filters = n_block, kernel_size = filter_size, kernel_initializer = filter_initializer, padding = 'same', strides=(2, 2)) (decoder)\n",
    "\n",
    "outp = Conv2DTranspose(filters=1, kernel_size = filter_size, activation = 'sigmoid', padding = 'same', kernel_initializer = keras.initializers.glorot_normal(seed=1337)) (decoder)\n",
    "model = Model(inputs = inp, outputs = outp)\n",
    "print(\"Model Layers: \" + str(int((len(model.layers)+1)/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 1.0):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(labels=y_true,predictions = y_pred_, num_classes = 2, weights = y_true) # Confusion matrix of [num_classes, num_classes]\n",
    "        backend.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return backend.mean(backend.stack(prec), axis=0)\n",
    "\n",
    "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        # Original binary crossentropy (see losses.py):\n",
    "        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce = backend.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "        \n",
    "        # Return the mean error\n",
    "        return backend.mean(weighted_b_ce)\n",
    "    return weighted_binary_crossentropy\n",
    "\n",
    "lr = 0.002\n",
    "optimizer = keras.optimizers.Adam(lr = lr)\n",
    "l1 = round(np.sum(training_labels) / (training_labels.shape[0] * training_labels.shape[1] * training_labels.shape[2]), 4)\n",
    "l2 = round(1 - l1, 4)\n",
    "loss = create_weighted_binary_crossentropy(l1, l2)\n",
    "metrics = [mean_iou]\n",
    "num_gpus = 1\n",
    "\n",
    "if num_gpus > 1:\n",
    "    self.model = multi_gpu_model(self.model, gpus=num_gpus)\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "model.name = \"MODEL\"\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "def_str = \"Ortho_NB2_\" + str(256) + \"_\" + str(blocks2) + \"_\" + str(lr) + \"_\" + str(num_color_images) + \"-\" + str(sigma) + \"_loss_\" + str(l1) + \"_\" + str(l2)\n",
    "print(def_str)\n",
    "\n",
    "tensorboard = TrainValTensorBoard(log_dir = \"logs/\" + def_str)\n",
    "filepath = \"weights/\" + def_str + \"-{epoch:02d}-{val_mean_iou:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mean_iou', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callback_list = [tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(training_images, training_labels, verbose=1, validation_split = 0.2, batch_size = 80, epochs = 15, initial_epoch = 0, callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights from .hdf5 file\n",
    "\n",
    "model.load_weights(\"final_NB2_256_[8, 16, 32, 64, 128]_0.002_0-1.5_loss_0.9434_0.0566-09-0.45.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to .h5 file\n",
    "\n",
    "model.save(def_str + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from .h5 file\n",
    "\n",
    "model = load_model(\"Ortho_NB2_256_[8, 16, 32, 64, 128]_0.002_1-1.5_loss_0.6701_0.3299-03-0.43.h5\", custom_objects={'mean_iou': mean_squared_error, 'weighted_binary_crossentropy': create_weighted_binary_crossentropy(0.5, 0.5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prediction\n",
    "\n",
    "prediction = model.predict(validation_images, verbose=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot metrics\n",
    "\n",
    "print(prediction.shape)\n",
    "val_labels = training_labels\n",
    "print(val_labels.shape)\n",
    "iou = []\n",
    "tp = []\n",
    "fp = []\n",
    "tn = []\n",
    "fn = []\n",
    "err = []\n",
    "accuracy = []\n",
    "xaxis = []\n",
    "for j in range(0, 100, 1):\n",
    "    xaxis.append(j / 100)\n",
    "    prediction2 = prediction > (j / 100)\n",
    "    \n",
    "    intersection = np.logical_and(prediction2 == 0, val_labels == 0)\n",
    "    union = ((1 - prediction2) + (1 - val_labels)) > 0\n",
    "    iou.append(np.sum(intersection) / np.sum(union))\n",
    "    \n",
    "    tn2 = np.sum(np.logical_and(prediction2 == val_labels, val_labels == 0))\n",
    "    fn2 = np.sum(np.logical_and(prediction2 != val_labels, val_labels == 1))\n",
    "    tp2 = np.sum(np.logical_and(prediction2 == val_labels, val_labels == 1))\n",
    "    fp2 = np.sum(np.logical_and(prediction2 != val_labels, val_labels == 0))\n",
    "    \n",
    "    tn.append(tp2 / (tp2+fn2))\n",
    "    fn.append(fp2 / (tn2+fp2))\n",
    "    tp.append(tn2 / (tn2+fp2))\n",
    "    fp.append(fn2 / (tp2+fn2))\n",
    "    \n",
    "    accuracy.append((tp2+tn2) / (tp2+tn2+fp2+fn2))\n",
    "    err.append((fp2+fn2)/(tp2+tn2+fn2+fp2))\n",
    "    print(j)\n",
    "\n",
    "print(\"Max IoU: \" + str(max(iou)) + \": \" + str(iou.index(max(iou))))\n",
    "print(\"Max TP: \" + str(max(tp)) + \": \" + str(tp.index(max(tp))))\n",
    "print(\"Max FP: \" + str(max(fp)) + \": \" + str(fp.index(max(fp))))\n",
    "print(\"Max TN: \" + str(max(tn)) + \": \" + str(tn.index(max(tn))))\n",
    "print(\"Max FN: \" + str(max(fn)) + \": \" + str(fn.index(max(fn))))\n",
    "print(\"Max Acc: \" + str(max(accuracy)) + \": \" + str(accuracy.index(max(accuracy))))\n",
    "print(\"Max Err: \" + str(max(err)) + \": \" + str(err.index(max(err))))\n",
    "\n",
    "figure(num=None, figsize=(10, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(xaxis, iou, label=\"IoU\")\n",
    "plt.plot(xaxis, tp, label=\"True Positive Rate\")\n",
    "plt.plot(xaxis, fp, label=\"False Positive Rate\")\n",
    "plt.plot(xaxis, tn, label=\"True negative Rate\")\n",
    "plt.plot(xaxis, fn, label=\"False negative Rate\")\n",
    "plt.plot(xaxis, accuracy, label=\"Accuracy\")\n",
    "plt.plot(xaxis, err, label=\"Error Rate\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate ROC curve plot\n",
    "\n",
    "figure(num=None, figsize=(10, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(fp, tp)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress tensorboard logs folder in order to download it\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "def recursive_files(dir_name='.', ignore=None):\n",
    "    for dir_name,subdirs,files in os.walk(dir_name):\n",
    "        if ignore and os.path.basename(dir_name) in ignore: \n",
    "            continue\n",
    "        for file_name in files:\n",
    "            if ignore and file_name in ignore:\n",
    "                continue\n",
    "            yield os.path.join(dir_name, file_name)\n",
    "def make_tar_file(dir_name='.', tar_file_name='tarfile.tar', ignore=None):\n",
    "    tar = tarfile.open(tar_file_name, 'w')\n",
    "    for file_name in recursive_files(dir_name, ignore):\n",
    "        tar.add(file_name)\n",
    "    tar.close()\n",
    "dir_name = 'logs'\n",
    "tar_file_name = 'logs/logs.tar'\n",
    "ignore = {'.ipynb_checkpoints', '__pycache__', tar_file_name}\n",
    "make_tar_file(dir_name, tar_file_name, ignore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
